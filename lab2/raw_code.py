# -*- coding: utf-8 -*-
"""lab2_espresso.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sQ9_qxx7B6NFQpo5nOwe1c3Bgx3-J86O
"""

# Link the drive
from google.colab import drive
drive.mount('/content/drive')

# espresso.py
from __future__ import annotations
from typing import List, Set, Dict, Tuple
import sys, os, argparse

# ============ Core helpers ============
def implicant_covers_input(cube: str, xb: str) -> bool:
    return all(c == '-' or c == x for c, x in zip(cube, xb))

def implicant_covers_implicant(a: str, b: str) -> bool:
    for ca, cb in zip(a, b):
        if ca != '-' and ca != cb:
            return False
    return True

def _merge_pair(a: str, b: str) -> Tuple[bool, str]:
    diff = 0
    out = []
    for ca, cb in zip(a, b):
        if ca == cb:
            out.append(ca)
        else:
            # only allow merge when both are fixed and different (QMC rule)
            if ca in '01' and cb in '01':
                diff += 1
                if diff > 1: return False, ''
                out.append('-')
            else:
                return False, ''
    return (diff == 1, ''.join(out))

def _reduce_absorb(cubes: List[str]) -> List[str]:
    uniq = sorted(set(cubes))
    keep = []
    for i, c in enumerate(uniq):
        if not any(i != j and implicant_covers_implicant(uniq[j], c) for j in range(len(uniq))):
            keep.append(c)
    return keep

def derive_prime_implicants(minterms: List[str]) -> List[str]:
    """Small QMC-like prime generator for minterms without '-'."""
    if not minterms: return []
    groups: Dict[int, Set[str]] = {}
    for m in set(minterms):
        groups.setdefault(m.count('1'), set()).add(m)

    used = set()
    primes: Set[str] = set()
    while groups:
        keys = sorted(groups.keys())
        next_groups: Dict[int, Set[str]] = {}
        merged_any = False
        used.clear()
        for k in keys:
            g1 = groups.get(k, set())
            g2 = groups.get(k+1, set())
            for a in g1:
                merged_here = False
                for b in g2:
                    ok, merged = _merge_pair(a, b)
                    if ok:
                        merged_here = True
                        merged_any = True
                        used.add(a); used.add(b)
                        next_groups.setdefault(merged.count('1'), set()).add(merged)
                if not merged_here and a not in used:
                    primes.add(a)
        # add untouched from g2
        for k in keys:
            for a in groups.get(k, set()):
                if a not in used:
                    primes.add(a)
        groups = next_groups if merged_any else {}
    return _reduce_absorb(list(primes))

# ============ Espresso REI ============
def cube_literals_positions(cube: str) -> List[int]:
    return [i for i, b in enumerate(cube) if b in "01"]

def apply_raises(cube: str, keep_positions: Set[int]) -> str:
    out = list(cube)
    for i, b in enumerate(out):
        if b in "01" and i not in keep_positions:
            out[i] = '-'
    return ''.join(out)

def build_off_cover(inputs_bits: List[str], onset_bits: Set[str], dcare_bits: Set[str]) -> List[str]:
    off_bits = list(set(inputs_bits) - (onset_bits | dcare_bits))
    if not off_bits:
        return []
    return derive_prime_implicants(off_bits)

def blocking_matrix_rows(cube: str) -> List[Tuple[int, str]]:
    return [(i, b) for i, b in enumerate(cube) if b in '01']

def blocking_cell_is_one(row: Tuple[int, str], off_cube: str) -> bool:
    pos, pol = row
    ob = off_cube[pos]
    if ob == '-': return False
    return ob != pol

def greedy_min_rows_cover(all_rows: List[Tuple[int, str]], off_cover: List[str]) -> Set[int]:
    if not off_cover: return set()
    cols = list(range(len(off_cover)))
    cover_map: Dict[int, Set[int]] = {}
    for r_idx, row in enumerate(all_rows):
        cover_map[r_idx] = {j for j, oc in enumerate(off_cover) if blocking_cell_is_one(row, oc)}
    uncovered = set(cols)
    picked: Set[int] = set()
    while uncovered:
        best, gain = None, 0
        for r_idx, colset in cover_map.items():
            if r_idx in picked: continue
            g = len(colset & uncovered)
            if g > gain:
                best, gain = r_idx, g
        if best is None:
            # no progress; pick any remaining row to avoid stall
            remaining = [r for r in range(len(all_rows)) if r not in picked]
            if not remaining: break
            best = remaining[0]
        picked.add(best)
        uncovered -= cover_map.get(best, set())
        if gain == 0 and not uncovered:
            break
    return picked

def expand_one_cube(cube: str, off_cover: List[str]) -> str:
    rows = blocking_matrix_rows(cube)
    if not rows or not off_cover:
        return '-' * len(cube)
    keep_row_ids = greedy_min_rows_cover(rows, off_cover)
    keep_positions = {rows[r][0] for r in keep_row_ids}
    return apply_raises(cube, keep_positions)

def irredundant(cover: List[str], inputs_bits: List[str], on_indices: Set[int]) -> List[str]:
    cov: Dict[str, Set[int]] = {c: set() for c in cover}
    for c in cover:
        for i in on_indices:
            if implicant_covers_input(c, inputs_bits[i]):
                cov[c].add(i)
    keep: List[str] = []
    for i, c in enumerate(cover):
        others = set()
        for j, d in enumerate(cover):
            if i == j: continue
            others |= cov[d]
        if not cov[c] <= others:
            keep.append(c)
    return keep

def reduce_cover(cover: List[str], inputs_bits: List[str], on_indices: Set[int]) -> List[str]:
    reduced = cover[:]
    on_to_cubes: Dict[int, Set[int]] = {i: set() for i in on_indices}
    for idx, c in enumerate(reduced):
        for i in on_indices:
            if implicant_covers_input(c, inputs_bits[i]):
                on_to_cubes[i].add(idx)
    def safe_to_raise(idx_cube: int, pos: int) -> bool:
      c = reduced[idx_cube]
      if c[pos] == '-': return True
      trial = c[:pos] + '-' + c[pos+1:]
      for i in on_indices:
          if idx_cube in on_to_cubes[i] and len(on_to_cubes[i]) == 1:
              if not implicant_covers_input(trial, inputs_bits[i]):
                  return False
      return True
    for idx in range(len(reduced)):
      c = reduced[idx]
      for pos in range(len(c)):
          if c[pos] in "01" and safe_to_raise(idx, pos):
              c = c[:pos] + '-' + c[pos+1:]
      reduced[idx] = c
    return reduced

def espresso_minimize_for_output(inputs_bits: List[str],
                                 outputs_trits: List[str],
                                 which_output: int,
                                 max_iters: int = 5) -> List[str]:
    onset_bits  = {x for x, y in zip(inputs_bits, outputs_trits) if y[which_output] == '1'}
    dcare_bits  = {x for x, y in zip(inputs_bits, outputs_trits) if y[which_output] == '-'}
    on_indices  = {i for i, (x, y) in enumerate(zip(inputs_bits, outputs_trits)) if y[which_output] == '1'}

    cover = derive_prime_implicants(sorted(onset_bits | dcare_bits)) if (onset_bits or dcare_bits) else []
    off_bits = set(inputs_bits) - (onset_bits | dcare_bits)
    if off_bits:
        cover = [c for c in cover if not any(implicant_covers_input(c, xb) for xb in off_bits)]

    off_cover = build_off_cover(inputs_bits, onset_bits, dcare_bits)

    for _ in range(max_iters):
        before = set(cover)
        cover = reduce_cover(cover, inputs_bits, on_indices)
        cover = [expand_one_cube(c, off_cover) for c in cover]
        cover = irredundant(cover, inputs_bits, on_indices)
        # absorption
        cleaned: List[str] = []
        for i, c in enumerate(cover):
            if not any(i != j and implicant_covers_implicant(cover[j], c) for j in range(len(cover))):
                cleaned.append(c)
        cover = sorted(set(cleaned))
        if set(cover) == before:
            break
    return cover

def cubes_for_pla(selected_cubes: List[str], n_outputs: int, which_output: int) -> List[str]:
    out = []
    for cube in selected_cubes:
        y = ['0'] * n_outputs
        y[which_output] = '1'
        out.append(f"{cube} {''.join(y)}")
    return out

# ---------- Sum-of-minterms format ----------
def read_sum_of_minterms_file(path: str):
    """
    Lines like:
      F1 = sum(1, 2, 5) + d(3, 4)
      F2 = sum(0, 7)
    - minterms 0-based
    - N = ceil(log2(max_index+1))
    """
    import re
    with open(path, 'r', encoding='utf-8') as f:
        lines = [ln.strip() for ln in f if ln.strip() and not ln.strip().startswith('#')]

    pat = re.compile(
        r'^\s*([A-Za-z]\w*)\s*=\s*sum\(\s*([0-9,\s]*)\s*\)\s*(?:\+\s*d\(\s*([0-9,\s]*)\s*\))?\s*$',
        re.IGNORECASE
    )

    outputs = []  # (name, on_set, dc_set)
    max_idx = -1

    def parse_list(s: str):
        s = (s or '').strip()
        if not s:
            return set()
        return {int(x.strip()) for x in s.split(',') if x.strip() != ''}

    for ln in lines:
        m = pat.match(ln)
        if not m:
            raise ValueError(f"Line not in 'F = sum(...) + d(...)' format: {ln}")
        name = m.group(1)
        on_set = parse_list(m.group(2))
        dc_set = parse_list(m.group(3))
        if on_set & dc_set:
            raise ValueError(f"Overlap in ON and d-care for {name}: {sorted(on_set & dc_set)}")
        if on_set:
            max_idx = max(max_idx, max(on_set))
        if dc_set:
            max_idx = max(max_idx, max(dc_set))
        outputs.append((name, on_set, dc_set))

    if max_idx < 0:
        raise ValueError("No minterms found in sum()/d().")

    # infer N
    N = 1
    while (1 << N) <= max_idx:
        N += 1

    in_names  = [f'x{i+1}' for i in range(N)]
    out_names = [name for (name, _, _) in outputs]
    M = len(out_names)

    # build full input space
    def bits(i: int) -> str:
        return format(i, f'0{N}b')

    inputs_bits   = [bits(i) for i in range(1 << N)]
    outputs_cols  = [['0'] * (1 << N) for _ in range(M)]

    for k, (_, on_set, dc_set) in enumerate(outputs):
        for i in range(1 << N):
            if i in dc_set:
                outputs_cols[k][i] = '-'
            elif i in on_set:
                outputs_cols[k][i] = '1'
            else:
                outputs_cols[k][i] = '0'

    outputs_trits = [''.join(outputs_cols[k][i] for k in range(M)) for i in range(1 << N)]
    return in_names, out_names, inputs_bits, outputs_trits

# ---------- Unified reader (always returns a 4-tuple) ----------
def read_input_file(path: str):
    """
    Tries, in order:
      1) sum-of-minterms format (F = sum(...) + d(...))
      2) JSON (list/dict with 'in'/'out' or 'inputs'/'outputs')
      3) PLA (.i .o .ilb .ob ... data ... .e)
      4) RAW lines: extract first two [01-]+ tokens per line
    Returns: (input_names, output_names, inputs_bits, outputs_trits)
    """
    import json, re

    with open(path, 'r', encoding='utf-8') as fchk:
        raw_txt = fchk.read()

    # 1) Detect sum-of-minterms
    if 'sum(' in raw_txt:
        return read_sum_of_minterms_file(path)

    # 2) Try JSON
    txt = raw_txt.strip()
    if txt and txt[0] in '{[':
        try:
            obj = json.loads(txt)
            def norm_item(item):
                iv = item.get('in') or item.get('inputs') or item.get('x') or item.get('X')
                ov = item.get('out') or item.get('outputs') or item.get('y') or item.get('Y')
                if iv is None: raise ValueError("JSON item missing 'in'/'inputs'")
                if ov is None: ov = '1'
                return str(iv), str(ov)

            if isinstance(obj, dict) and 'data' in obj and isinstance(obj['data'], list):
                pairs = [norm_item(it) for it in obj['data']]
            elif isinstance(obj, list):
                pairs = [norm_item(it) for it in obj]
            elif isinstance(obj, dict) and ('in' in obj or 'inputs' in obj):
                pairs = [norm_item(obj)]
            else:
                raise ValueError("Unrecognized JSON structure")

            t_in  = max(len(iv) for iv, _ in pairs)
            t_out = max(len(ov) for _, ov in pairs)
            in_names  = [f"x{i+1}" for i in range(t_in)]
            out_names = [f"f{i+1}" for i in range(t_out)]

            norm_pairs = []
            for iv, ov in pairs:
                if any(ch not in '01-' for ch in iv) or any(ch not in '01-' for ch in ov):
                    raise ValueError(f"Invalid symbol in JSON item: {iv} {ov}")
                if iv == '-':
                    iv = '-' * t_in
                if len(iv) < t_in:
                    iv = iv + ('-' * (t_in - len(iv)))
                if len(ov) < t_out:
                    ov = ov + ('-' * (t_out - len(ov)))
                norm_pairs.append((iv, ov))

            inputs_bits   = [iv.replace('-', '0') for iv, _ in norm_pairs]
            outputs_trits = [ov for _, ov in norm_pairs]
            return in_names, out_names, inputs_bits, outputs_trits
        except Exception:
            pass  # fall through

    # 3/4) PLA / RAW
    lines = [ln.strip() for ln in raw_txt.splitlines() if ln.strip() and not ln.strip().startswith('#')]

    n_in_decl  = None
    n_out_decl = None
    in_names:  list[str] = []
    out_names: list[str] = []
    data_pairs: list[tuple[str, str]] = []
    tokpat = re.compile(r'[01-]+')

    for line in lines:
        if line.startswith('.'):
            toks = line.split()
            key = toks[0]
            if key == '.i' and len(toks) >= 2:
                n_in_decl = int(toks[1])
            elif key == '.o' and len(toks) >= 2:
                n_out_decl = int(toks[1])
            elif key == '.ilb':
                in_names = toks[1:]
            elif key == '.ob':
                out_names = toks[1:]
            # ignore others
        else:
            bits = tokpat.findall(line)
            if not bits:
                continue
            if len(bits) == 1:
                data_pairs.append((bits[0], '1'))
            else:
                data_pairs.append((bits[0], bits[1]))

    if not data_pairs:
        raise ValueError("No data lines recognized in input file.")

    max_in_seen  = max(len(iv) for iv, _ in data_pairs)
    max_out_seen = max(len(ov) for _, ov in data_pairs)
    t_in  = n_in_decl  if n_in_decl  is not None else max_in_seen
    t_out = n_out_decl if n_out_decl is not None else max_out_seen

    if not in_names:
        in_names  = [f"x{i+1}" for i in range(t_in)]
    if not out_names:
        out_names = [f"f{i+1}" for i in range(t_out)]

    norm_pairs: list[tuple[str, str]] = []
    for iv, ov in data_pairs:
        if any(ch not in '01-' for ch in iv) or any(ch not in '01-' for ch in ov):
            raise ValueError(f"Invalid symbol in line: {iv} {ov}")
        if iv == '-':
            iv = '-' * t_in
        if len(iv) < t_in:
            iv = iv + ('-' * (t_in - len(iv)))
        if len(ov) < t_out:
            ov = ov + ('-' * (t_out - len(ov)))
        if len(iv) > t_in or len(ov) > t_out:
            raise ValueError(f"Token longer than target size: {iv} {ov} (targets {t_in},{t_out})")
        norm_pairs.append((iv, ov))

    inputs_bits   = [iv.replace('-', '0') for iv, _ in norm_pairs]
    outputs_trits = [ov for _, ov in norm_pairs]
    return in_names, out_names, inputs_bits, outputs_trits

def write_pla_file(path: str, n_inputs: int, n_outputs: int, lines: List[str],
                   input_labels: List[str] | None = None,
                   output_labels: List[str] | None = None):
    with open(path, 'w', encoding='utf-8') as f:
        f.write(f".i {n_inputs}\n")
        f.write(f".o {n_outputs}\n")
        if input_labels:
            f.write(".ilb " + " ".join(input_labels) + "\n")
        if output_labels:
            f.write(".ob " + " ".join(output_labels) + "\n")
        for ln in lines:
            f.write(ln + "\n")
        f.write(".e\n")

# ============ Pretty print ============
def cube_to_term(cube: str, var_names: List[str]) -> str:
    term = []
    for b, n in zip(cube, var_names):
        if b == '1': term.append(n)
        elif b == '0': term.append(n + "'")
    return ''.join(term) if term else "1"

# ===============================================================
# ===  SOP (sum-of-minterms) random generator  ==================
# ===============================================================
import random

def gen_random_sop(N: int,
                   M: int,
                   on_ratio: float = 0.30,
                   dc_ratio: float = 0.05,
                   seed: int | None = None,
                   cap_half: bool = True) -> list[str]:
    """
    Generate M random functions in SOP-spec format:
        Fi = sum(<minterm indices>) + d(<dc indices>)
    - 0-based minterm indexing over full space of 2^N
    - on_ratio, dc_ratio = fractions of full space
    - cap_half: limit |ON| ≤ 2^(N-1)
    """
    if N <= 0 or M <= 0:
        raise ValueError("N and M must be positive.")
    U = 1 << N  # total minterms
    rnd = random.Random(seed)
    lines = []
    for k in range(1, M + 1):
        on_target = max(0, min(U, round(on_ratio * U)))
        dc_target = max(0, min(U, round(dc_ratio * U)))
        if cap_half:
            on_target = min(on_target, U // 2)

        universe = list(range(U))
        rnd.shuffle(universe)

        on_set = set(universe[:on_target])
        rem = [x for x in universe[on_target:] if x not in on_set]
        dc_set = set(rem[:max(0, min(dc_target, U - len(on_set)))])

        on_list = sorted(on_set)
        dc_list = sorted(dc_set)

        sum_part = f"sum({', '.join(str(i) for i in on_list)})" if on_list else "sum()"
        if dc_list:
            line = f"F{k} = {sum_part} + d(" + ", ".join(str(i) for i in dc_list) + ")"
        else:
            line = f"F{k} = {sum_part}"
        lines.append(line)
    return lines


def save_sop_file(path: str, lines: list[str]) -> None:
    with open(path, "w", encoding="utf-8") as f:
        for ln in lines:
            f.write(ln + "\n")
    print(f"[GEN] SOP spec saved to: {path}")

# ============ Main ============
def main_es(input_path: str, output_path: str):
    in_names, out_names, inputs_bits, outputs_trits = read_input_file(input_path)
    print(f"[INFO] Loaded: rows={len(inputs_bits)}, inputs={len(in_names)}, outputs={len(out_names)}")

    pla_lines: List[str] = []
    for k, out_name in enumerate(out_names):
        print(f"\n--- Minimizing output {out_name} ---")
        cubes = espresso_minimize_for_output(inputs_bits, outputs_trits, which_output=k)
        print(f"[INFO] {len(cubes)} cubes:")
        for c in cubes:
            print("  " + cube_to_term(c, in_names))
        pla_lines.extend(cubes_for_pla(cubes, len(out_names), k))

    write_pla_file(output_path, len(in_names), len(out_names), pla_lines, in_names, out_names)
    print(f"\n[RESULT] Saved PLA to: {output_path}")
    print("[DONE]")

# ===============================================================
# ===  CLI entry point ==========================================
# ===============================================================
def main_cli():
    """
    Command-line interface for the Espresso Standalone tool.
    Supports:
      - Running Espresso minimization on an existing input file
      - Generating a random SOP spec first (--gen)
    """
    parser = argparse.ArgumentParser(description="Espresso minimizer (REI) + SOP generator.")
    parser.add_argument("input", nargs="?", default="kmap_file.txt",
                        help="Input file (SOP, PLA, or truth table). Use --gen to generate one.")
    parser.add_argument("-o", "--output", default="espresso_output.pla",
                        help="Output PLA file path.")

    # Generator options
    parser.add_argument("--gen", action="store_true",
                        help="Generate a random SOP spec before running Espresso.")
    parser.add_argument("-n", "--inputs", type=int, default=3,
                        help="Number of input variables (used with --gen).")
    parser.add_argument("-m", "--functions", type=int, default=1,
                        help="Number of output functions (used with --gen).")
    parser.add_argument("-p", "--on_ratio", type=float, default=0.30,
                        help="Fraction of ON-set minterms (used with --gen).")
    parser.add_argument("-d", "--dc_ratio", type=float, default=0.05,
                        help="Fraction of don't-care minterms (used with --gen).")
    parser.add_argument("--seed", type=int, default=None,
                        help="Random seed for reproducibility (used with --gen).")
    parser.add_argument("--no-cap-half", action="store_true",
                        help="Do NOT cap |ON| ≤ half of the space (default: capped).")

    # parse_known_args() ignores Jupyter's -f kernel arg
    args, _ = parser.parse_known_args()

    input_path = args.input
    if args.gen:
        # Generate random SOP input before running Espresso
        input_path = "spec_sop.txt"
        sop_lines = gen_random_sop(
            N=args.inputs,
            M=args.functions,
            on_ratio=args.on_ratio,
            dc_ratio=args.dc_ratio,
            seed=args.seed,
            cap_half=(not args.no_cap_half)
        )
        save_sop_file(input_path, sop_lines)

    if not os.path.exists(input_path):
        print(f"Error: input file '{input_path}' not found.")
        sys.exit(1)

    # Run Espresso flow
    main_es(input_path, args.output)


# ===============================================================
# ===  Entry point ==============================================
# ===============================================================
if __name__ == "__main__":
    main_cli()

from __future__ import annotations
import sys
import os
from io import StringIO
from contextlib import redirect_stdout
from typing import List, Optional

# === keep your existing IO helpers/modules ===
from truth_table import (
    parse_sum_of_minterms_file,
    build_outputs_from_minterm_indices,
    print_truth_table,              # console (plain text)
    print_truth_table_markdown,     # used only to SAVE a .md table
    generate_random_spec_file,
)
# REPLACE the old cover import: we won't use select_cover_for_one_output anymore
# from cover import select_cover_for_one_output
from pla import build_full_pla

# === add Espresso minimizer import (use your espresso.py) ===
from espresso import espresso_minimize_for_output


# ---------- local helpers to keep your output format ----------
def _safe_stem(path: str) -> str:
    base = os.path.basename(path)
    if "." in base:
        return ".".join(base.split(".")[:-1]) or base
    return base or "run"

def _save_truth_table_markdown(
    inputs_bits: List[str],
    outputs_trits: List[str],
    input_names: List[str],
    output_names: List[str],
    out_path: str,
) -> None:
    """Capture the markdown table and write it to `out_path`."""
    buf = StringIO()
    with redirect_stdout(buf):
        print_truth_table_markdown(inputs_bits, outputs_trits, input_names, output_names)
    md = buf.getvalue()
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(md)

def _cube_to_sop_term(cube: str, var_names: List[str]) -> str:
    """Convert PCN cube like '10-' with vars A,B,C -> SOP literal string 'A B''."""
    parts = []
    for b, n in zip(cube, var_names):
        if b == '1': parts.append(n)
        elif b == '0': parts.append(n + "'")
    return ''.join(parts) if parts else "1"

def _cubes_for_pla(selected_cubes: List[str], n_outputs: int, which_output: int) -> List[str]:
    """Map list of cubes into PLA rows '<cube> <yy..y>' with single '1' at which_output."""
    rows = []
    for cube in selected_cubes:
        y = ['0'] * n_outputs
        y[which_output] = '1'
        rows.append(f"{cube} {''.join(y)}")
    return rows


# ---------- your flow, but Espresso inside ----------
def run_from_sum_file(
    path: str,
    n_inputs: int,
    input_names: Optional[List[str]] = None,
    use_markdown: bool = False,  # if True -> save ONLY the truth table in Markdown
) -> str:
    # Parse spec and build truth table
    spec = parse_sum_of_minterms_file(path)  # name -> (on_set, dc_set)
    inputs_bits, outputs_trits, out_names = build_outputs_from_minterm_indices(n_inputs, spec)

    if input_names is None:
        input_names = [f"x{i+1}" for i in range(n_inputs)]
    output_names = out_names

    # Console: always plain text truth table
    print("Truth table (plain):")
    print_truth_table(inputs_bits, outputs_trits, input_names, output_names)

    # Optionally save ONLY the truth table as Markdown
    if use_markdown:
        stem = _safe_stem(path)
        md_path = f"{stem}_truth_table.md"
        _save_truth_table_markdown(inputs_bits, outputs_trits, input_names, output_names, md_path)
        print(f"[i] Markdown truth table saved to: {md_path}")

    # === Espresso cover for each output (keeps your console format) ===
    all_pla_rows: List[str] = []
    for k, out_name in enumerate(output_names):
        # Minimize via Espresso (REI loop)
        es_cubes: List[str] = espresso_minimize_for_output(
            inputs_bits=inputs_bits,
            outputs_trits=outputs_trits,
            which_output=k,
        )

        # SOP string for console
        sop_terms = [_cube_to_sop_term(c, input_names) for c in es_cubes]
        sop_str = " + ".join(sop_terms) if sop_terms else "0"

        print(f"\n=== {out_name} ===")
        print("PIs:", es_cubes)   # show cubes as implicants (expanded cover)
        print("SOP:", sop_str)

        # Collect PLA rows for this output
        all_pla_rows.extend(_cubes_for_pla(es_cubes, n_outputs=len(output_names), which_output=k))

    # Build and print PLA (plain text) using your existing function
    pla_text = build_full_pla(inputs_bits, outputs_trits, all_pla_rows, input_names, output_names)
    print("\nPLA:")
    print(pla_text)

    return pla_text


def print_usage():
    print(
        "Usage:\n"
        "  python3 main.py random [N] [M] [on_ratio] [dc_ratio]\n"
        "      -> Generate random_spec.txt then run it.\n"
        "         Defaults: N=4, M=2, on_ratio=0.35 (clamped ≤0.5), dc_ratio=0.15\n"
        "         Set use_markdown=True in code to also save the truth table as Markdown.\n"
        "\n"
        "  python3 main.py spec.txt [N]\n"
        "      -> Run with provided spec file (supports d{...}); default N=3.\n"
    )


def main():
    args = sys.argv[1:]
    if not args:
        print_usage()
        return

    mode = args[0].strip()
    use_markdown = True   # toggle: True saves ONLY the truth table .md file

    if mode.lower() == "random":
        try:
            N  = int(args[1]) if len(args) >= 2 else 4
            M  = int(args[2]) if len(args) >= 3 else 2
            on = float(args[3]) if len(args) >= 4 else 0.35
            dc = float(args[4]) if len(args) >= 5 else 0.15
        except ValueError:
            print("Error: parameters must be numeric (N,M ints; on_ratio, dc_ratio floats).")
            return

        if on > 0.5:
            print("Warning: on_ratio > 0.5 is clamped to 0.5.")
            on = 0.5

        out_path = "random_spec.txt"
        generate_random_spec_file(
            out_path, n_inputs=N, n_outputs=M,
            on_ratio=on, dc_ratio=dc, ensure_on=True, seed=None
        )
        print(f"[i] Generated random spec -> {out_path}")
        run_from_sum_file(path=out_path, n_inputs=N, use_markdown=use_markdown)
        return

    # spec file: python3 main.py spec.txt [N]
    path = mode
    try:
        N = int(args[1]) if len(args) >= 2 else 3
    except ValueError:
        print("Error: N must be an integer.")
        return

    run_from_sum_file(path=path, n_inputs=N, use_markdown=use_markdown)


if __name__ == "__main__":
    main()